{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. FCN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YqkKav8O6Fh1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/jupyter/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/envs/jupyter/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import os.path as osp\n",
    "import PIL\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torchsummary import summary\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "if torch.cuda.is_available(): device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Downloading dataset from google drive, 밑의 코드의 주석을 풀면 구글 드라이브로 부터 데이터셋 다운, 압축이 풀리고, Kitti라는 폴더가 생성됩니다. \n",
    "\n",
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=19EiycfOQtf6uDKvMgwlHZB50cAxX_U4z' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=19EiycfOQtf6uDKvMgwlHZB50cAxX_U4z\" -O Kitti.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip Kitti.zip -d ./data/Kitti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "error",
     "timestamp": 1636096005436,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "TwA5VAjp6Fh7",
    "outputId": "24ad80d0-4595-45b5-b8c4-c7b3da462966",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/image_2/um_000000.png training/gt_image_2/um_road_000000.png\n",
      "training/image_2/um_000001.png training/gt_image_2/um_road_000001.png\n",
      "training/image_2/um_000002.png training/gt_image_2/um_road_000002.png\n",
      "training/image_2/um_000003.png training/gt_image_2/um_road_000003.png\n",
      "training/image_2/um_000004.png training/gt_image_2/um_road_000004.png\n",
      "training/image_2/um_000005.png training/gt_image_2/um_road_000005.png\n",
      "training/image_2/um_000006.png training/gt_image_2/um_road_000006.png\n",
      "training/image_2/um_000007.png training/gt_image_2/um_road_000007.png\n",
      "training/image_2/um_000008.png training/gt_image_2/um_road_000008.png\n",
      "training/image_2/um_000009.png training/gt_image_2/um_road_000009.png\n",
      "training/image_2/um_000010.png training/gt_image_2/um_road_000010.png\n",
      "training/image_2/um_000011.png training/gt_image_2/um_road_000011.png\n",
      "training/image_2/um_000012.png training/gt_image_2/um_road_000012.png\n",
      "training/image_2/um_000013.png training/gt_image_2/um_road_000013.png\n",
      "training/image_2/um_000014.png training/gt_image_2/um_road_000014.png\n",
      "training/image_2/um_000015.png training/gt_image_2/um_road_000015.png\n",
      "training/image_2/um_000016.png training/gt_image_2/um_road_000016.png\n",
      "training/image_2/um_000017.png training/gt_image_2/um_road_000017.png\n",
      "training/image_2/um_000018.png training/gt_image_2/um_road_000018.png\n",
      "training/image_2/um_000019.png training/gt_image_2/um_road_000019.png\n",
      "training/image_2/um_000020.png training/gt_image_2/um_road_000020.png\n",
      "training/image_2/um_000021.png training/gt_image_2/um_road_000021.png\n",
      "training/image_2/um_000022.png training/gt_image_2/um_road_000022.png\n",
      "training/image_2/um_000023.png training/gt_image_2/um_road_000023.png\n",
      "training/image_2/um_000024.png training/gt_image_2/um_road_000024.png\n",
      "training/image_2/um_000025.png training/gt_image_2/um_road_000025.png\n",
      "training/image_2/um_000026.png training/gt_image_2/um_road_000026.png\n",
      "training/image_2/um_000027.png training/gt_image_2/um_road_000027.png\n",
      "training/image_2/um_000028.png training/gt_image_2/um_road_000028.png\n",
      "training/image_2/um_000029.png training/gt_image_2/um_road_000029.png\n",
      "training/image_2/um_000030.png training/gt_image_2/um_road_000030.png\n",
      "training/image_2/um_000031.png training/gt_image_2/um_road_000031.png\n",
      "training/image_2/um_000032.png training/gt_image_2/um_road_000032.png\n",
      "training/image_2/um_000033.png training/gt_image_2/um_road_000033.png\n",
      "training/image_2/um_000034.png training/gt_image_2/um_road_000034.png\n",
      "training/image_2/um_000035.png training/gt_image_2/um_road_000035.png\n",
      "training/image_2/um_000036.png training/gt_image_2/um_road_000036.png\n",
      "training/image_2/um_000037.png training/gt_image_2/um_road_000037.png\n",
      "training/image_2/um_000038.png training/gt_image_2/um_road_000038.png\n",
      "training/image_2/um_000039.png training/gt_image_2/um_road_000039.png\n",
      "training/image_2/um_000040.png training/gt_image_2/um_road_000040.png\n",
      "training/image_2/um_000041.png training/gt_image_2/um_road_000041.png\n",
      "training/image_2/um_000042.png training/gt_image_2/um_road_000042.png\n",
      "training/image_2/um_000043.png training/gt_image_2/um_road_000043.png\n",
      "training/image_2/um_000044.png training/gt_image_2/um_road_000044.png\n",
      "training/image_2/um_000045.png training/gt_image_2/um_road_000045.png\n",
      "training/image_2/um_000046.png training/gt_image_2/um_road_000046.png\n",
      "training/image_2/um_000047.png training/gt_image_2/um_road_000047.png\n",
      "training/image_2/um_000048.png training/gt_image_2/um_road_000048.png\n",
      "training/image_2/um_000049.png training/gt_image_2/um_road_000049.png\n",
      "training/image_2/um_000050.png training/gt_image_2/um_road_000050.png\n",
      "training/image_2/um_000051.png training/gt_image_2/um_road_000051.png\n",
      "training/image_2/um_000052.png training/gt_image_2/um_road_000052.png\n",
      "training/image_2/um_000053.png training/gt_image_2/um_road_000053.png\n",
      "training/image_2/um_000054.png training/gt_image_2/um_road_000054.png\n",
      "training/image_2/um_000055.png training/gt_image_2/um_road_000055.png\n",
      "training/image_2/um_000056.png training/gt_image_2/um_road_000056.png\n",
      "training/image_2/um_000057.png training/gt_image_2/um_road_000057.png\n",
      "training/image_2/um_000058.png training/gt_image_2/um_road_000058.png\n",
      "training/image_2/um_000059.png training/gt_image_2/um_road_000059.png\n",
      "training/image_2/um_000060.png training/gt_image_2/um_road_000060.png\n",
      "training/image_2/um_000061.png training/gt_image_2/um_road_000061.png\n",
      "training/image_2/um_000062.png training/gt_image_2/um_road_000062.png\n",
      "training/image_2/um_000063.png training/gt_image_2/um_road_000063.png\n",
      "training/image_2/um_000064.png training/gt_image_2/um_road_000064.png\n",
      "training/image_2/um_000065.png training/gt_image_2/um_road_000065.png\n",
      "training/image_2/um_000066.png training/gt_image_2/um_road_000066.png\n",
      "training/image_2/um_000067.png training/gt_image_2/um_road_000067.png\n",
      "training/image_2/um_000068.png training/gt_image_2/um_road_000068.png\n",
      "training/image_2/um_000069.png training/gt_image_2/um_road_000069.png\n",
      "training/image_2/um_000070.png training/gt_image_2/um_road_000070.png\n",
      "training/image_2/um_000071.png training/gt_image_2/um_road_000071.png\n",
      "training/image_2/um_000072.png training/gt_image_2/um_road_000072.png\n",
      "training/image_2/um_000073.png training/gt_image_2/um_road_000073.png\n",
      "training/image_2/um_000074.png training/gt_image_2/um_road_000074.png\n",
      "training/image_2/um_000075.png training/gt_image_2/um_road_000075.png\n",
      "training/image_2/um_000076.png training/gt_image_2/um_road_000076.png\n",
      "training/image_2/um_000077.png training/gt_image_2/um_road_000077.png\n",
      "training/image_2/um_000078.png training/gt_image_2/um_road_000078.png\n",
      "training/image_2/umm_000000.png training/gt_image_2/umm_road_000000.png\n",
      "training/image_2/umm_000001.png training/gt_image_2/umm_road_000001.png\n",
      "training/image_2/umm_000002.png training/gt_image_2/umm_road_000002.png\n",
      "training/image_2/umm_000003.png training/gt_image_2/umm_road_000003.png\n",
      "training/image_2/umm_000004.png training/gt_image_2/umm_road_000004.png\n",
      "training/image_2/umm_000005.png training/gt_image_2/umm_road_000005.png\n",
      "training/image_2/umm_000006.png training/gt_image_2/umm_road_000006.png\n",
      "training/image_2/umm_000007.png training/gt_image_2/umm_road_000007.png\n",
      "training/image_2/umm_000008.png training/gt_image_2/umm_road_000008.png\n",
      "training/image_2/umm_000009.png training/gt_image_2/umm_road_000009.png\n",
      "training/image_2/umm_000010.png training/gt_image_2/umm_road_000010.png\n",
      "training/image_2/umm_000011.png training/gt_image_2/umm_road_000011.png\n",
      "training/image_2/umm_000012.png training/gt_image_2/umm_road_000012.png\n",
      "training/image_2/umm_000013.png training/gt_image_2/umm_road_000013.png\n",
      "training/image_2/umm_000014.png training/gt_image_2/umm_road_000014.png\n",
      "training/image_2/umm_000015.png training/gt_image_2/umm_road_000015.png\n",
      "training/image_2/umm_000016.png training/gt_image_2/umm_road_000016.png\n",
      "training/image_2/umm_000017.png training/gt_image_2/umm_road_000017.png\n",
      "training/image_2/umm_000018.png training/gt_image_2/umm_road_000018.png\n",
      "training/image_2/umm_000019.png training/gt_image_2/umm_road_000019.png\n",
      "training/image_2/umm_000020.png training/gt_image_2/umm_road_000020.png\n",
      "training/image_2/umm_000021.png training/gt_image_2/umm_road_000021.png\n",
      "training/image_2/umm_000022.png training/gt_image_2/umm_road_000022.png\n",
      "training/image_2/umm_000023.png training/gt_image_2/umm_road_000023.png\n",
      "training/image_2/umm_000024.png training/gt_image_2/umm_road_000024.png\n",
      "training/image_2/umm_000025.png training/gt_image_2/umm_road_000025.png\n",
      "training/image_2/umm_000026.png training/gt_image_2/umm_road_000026.png\n",
      "training/image_2/umm_000027.png training/gt_image_2/umm_road_000027.png\n",
      "training/image_2/umm_000028.png training/gt_image_2/umm_road_000028.png\n",
      "training/image_2/umm_000029.png training/gt_image_2/umm_road_000029.png\n",
      "training/image_2/umm_000030.png training/gt_image_2/umm_road_000030.png\n",
      "training/image_2/umm_000031.png training/gt_image_2/umm_road_000031.png\n",
      "training/image_2/umm_000032.png training/gt_image_2/umm_road_000032.png\n",
      "training/image_2/umm_000033.png training/gt_image_2/umm_road_000033.png\n",
      "training/image_2/umm_000034.png training/gt_image_2/umm_road_000034.png\n",
      "training/image_2/umm_000035.png training/gt_image_2/umm_road_000035.png\n",
      "training/image_2/umm_000036.png training/gt_image_2/umm_road_000036.png\n",
      "training/image_2/umm_000037.png training/gt_image_2/umm_road_000037.png\n",
      "training/image_2/umm_000038.png training/gt_image_2/umm_road_000038.png\n",
      "training/image_2/umm_000039.png training/gt_image_2/umm_road_000039.png\n",
      "training/image_2/umm_000040.png training/gt_image_2/umm_road_000040.png\n",
      "training/image_2/umm_000041.png training/gt_image_2/umm_road_000041.png\n",
      "training/image_2/umm_000042.png training/gt_image_2/umm_road_000042.png\n",
      "training/image_2/umm_000043.png training/gt_image_2/umm_road_000043.png\n",
      "training/image_2/umm_000044.png training/gt_image_2/umm_road_000044.png\n",
      "training/image_2/umm_000045.png training/gt_image_2/umm_road_000045.png\n",
      "training/image_2/umm_000046.png training/gt_image_2/umm_road_000046.png\n",
      "training/image_2/umm_000047.png training/gt_image_2/umm_road_000047.png\n",
      "training/image_2/umm_000048.png training/gt_image_2/umm_road_000048.png\n",
      "training/image_2/umm_000049.png training/gt_image_2/umm_road_000049.png\n",
      "training/image_2/umm_000050.png training/gt_image_2/umm_road_000050.png\n",
      "training/image_2/umm_000051.png training/gt_image_2/umm_road_000051.png\n",
      "training/image_2/umm_000052.png training/gt_image_2/umm_road_000052.png\n",
      "training/image_2/umm_000053.png training/gt_image_2/umm_road_000053.png\n",
      "training/image_2/umm_000054.png training/gt_image_2/umm_road_000054.png\n",
      "training/image_2/umm_000055.png training/gt_image_2/umm_road_000055.png\n",
      "training/image_2/umm_000056.png training/gt_image_2/umm_road_000056.png\n",
      "training/image_2/umm_000057.png training/gt_image_2/umm_road_000057.png\n",
      "training/image_2/umm_000058.png training/gt_image_2/umm_road_000058.png\n",
      "training/image_2/umm_000059.png training/gt_image_2/umm_road_000059.png\n",
      "training/image_2/umm_000060.png training/gt_image_2/umm_road_000060.png\n",
      "training/image_2/umm_000061.png training/gt_image_2/umm_road_000061.png\n",
      "training/image_2/umm_000062.png training/gt_image_2/umm_road_000062.png\n",
      "training/image_2/umm_000063.png training/gt_image_2/umm_road_000063.png\n",
      "training/image_2/umm_000064.png training/gt_image_2/umm_road_000064.png\n",
      "training/image_2/umm_000065.png training/gt_image_2/umm_road_000065.png\n",
      "training/image_2/umm_000066.png training/gt_image_2/umm_road_000066.png\n",
      "training/image_2/umm_000067.png training/gt_image_2/umm_road_000067.png\n",
      "training/image_2/umm_000068.png training/gt_image_2/umm_road_000068.png\n",
      "training/image_2/umm_000069.png training/gt_image_2/umm_road_000069.png\n",
      "training/image_2/umm_000070.png training/gt_image_2/umm_road_000070.png\n",
      "training/image_2/umm_000071.png training/gt_image_2/umm_road_000071.png\n",
      "training/image_2/umm_000072.png training/gt_image_2/umm_road_000072.png\n",
      "training/image_2/umm_000073.png training/gt_image_2/umm_road_000073.png\n",
      "training/image_2/umm_000074.png training/gt_image_2/umm_road_000074.png\n",
      "training/image_2/umm_000075.png training/gt_image_2/umm_road_000075.png\n",
      "training/image_2/umm_000076.png training/gt_image_2/umm_road_000076.png\n",
      "training/image_2/umm_000077.png training/gt_image_2/umm_road_000077.png\n",
      "training/image_2/umm_000078.png training/gt_image_2/umm_road_000078.png\n",
      "training/image_2/umm_000079.png training/gt_image_2/umm_road_000079.png\n",
      "training/image_2/uu_000000.png training/gt_image_2/uu_road_000000.png\n",
      "training/image_2/uu_000001.png training/gt_image_2/uu_road_000001.png\n",
      "training/image_2/uu_000002.png training/gt_image_2/uu_road_000002.png\n",
      "training/image_2/uu_000003.png training/gt_image_2/uu_road_000003.png\n",
      "training/image_2/uu_000004.png training/gt_image_2/uu_road_000004.png\n",
      "training/image_2/uu_000005.png training/gt_image_2/uu_road_000005.png\n",
      "training/image_2/uu_000006.png training/gt_image_2/uu_road_000006.png\n",
      "training/image_2/uu_000007.png training/gt_image_2/uu_road_000007.png\n",
      "training/image_2/uu_000008.png training/gt_image_2/uu_road_000008.png\n",
      "training/image_2/uu_000009.png training/gt_image_2/uu_road_000009.png\n",
      "training/image_2/uu_000010.png training/gt_image_2/uu_road_000010.png\n",
      "training/image_2/uu_000011.png training/gt_image_2/uu_road_000011.png\n",
      "training/image_2/uu_000012.png training/gt_image_2/uu_road_000012.png\n",
      "training/image_2/uu_000013.png training/gt_image_2/uu_road_000013.png\n",
      "training/image_2/uu_000014.png training/gt_image_2/uu_road_000014.png\n",
      "training/image_2/uu_000015.png training/gt_image_2/uu_road_000015.png\n",
      "training/image_2/uu_000016.png training/gt_image_2/uu_road_000016.png\n",
      "training/image_2/uu_000017.png training/gt_image_2/uu_road_000017.png\n",
      "training/image_2/uu_000018.png training/gt_image_2/uu_road_000018.png\n",
      "training/image_2/uu_000019.png training/gt_image_2/uu_road_000019.png\n",
      "training/image_2/uu_000020.png training/gt_image_2/uu_road_000020.png\n",
      "training/image_2/uu_000021.png training/gt_image_2/uu_road_000021.png\n",
      "training/image_2/uu_000022.png training/gt_image_2/uu_road_000022.png\n",
      "training/image_2/uu_000023.png training/gt_image_2/uu_road_000023.png\n",
      "training/image_2/uu_000024.png training/gt_image_2/uu_road_000024.png\n",
      "training/image_2/uu_000025.png training/gt_image_2/uu_road_000025.png\n",
      "training/image_2/uu_000026.png training/gt_image_2/uu_road_000026.png\n",
      "training/image_2/uu_000027.png training/gt_image_2/uu_road_000027.png\n",
      "training/image_2/uu_000028.png training/gt_image_2/uu_road_000028.png\n",
      "training/image_2/uu_000029.png training/gt_image_2/uu_road_000029.png\n",
      "training/image_2/uu_000030.png training/gt_image_2/uu_road_000030.png\n",
      "training/image_2/uu_000031.png training/gt_image_2/uu_road_000031.png\n",
      "training/image_2/uu_000032.png training/gt_image_2/uu_road_000032.png\n",
      "training/image_2/uu_000033.png training/gt_image_2/uu_road_000033.png\n",
      "training/image_2/uu_000034.png training/gt_image_2/uu_road_000034.png\n",
      "training/image_2/uu_000035.png training/gt_image_2/uu_road_000035.png\n",
      "training/image_2/uu_000036.png training/gt_image_2/uu_road_000036.png\n",
      "training/image_2/uu_000037.png training/gt_image_2/uu_road_000037.png\n",
      "training/image_2/uu_000038.png training/gt_image_2/uu_road_000038.png\n",
      "training/image_2/uu_000039.png training/gt_image_2/uu_road_000039.png\n",
      "training/image_2/uu_000040.png training/gt_image_2/uu_road_000040.png\n",
      "training/image_2/uu_000041.png training/gt_image_2/uu_road_000041.png\n",
      "training/image_2/uu_000042.png training/gt_image_2/uu_road_000042.png\n",
      "training/image_2/uu_000043.png training/gt_image_2/uu_road_000043.png\n",
      "training/image_2/uu_000044.png training/gt_image_2/uu_road_000044.png\n",
      "training/image_2/uu_000045.png training/gt_image_2/uu_road_000045.png\n",
      "training/image_2/uu_000046.png training/gt_image_2/uu_road_000046.png\n",
      "training/image_2/uu_000047.png training/gt_image_2/uu_road_000047.png\n",
      "training/image_2/uu_000048.png training/gt_image_2/uu_road_000048.png\n",
      "training/image_2/uu_000049.png training/gt_image_2/uu_road_000049.png\n",
      "training/image_2/uu_000050.png training/gt_image_2/uu_road_000050.png\n",
      "training/image_2/uu_000051.png training/gt_image_2/uu_road_000051.png\n",
      "training/image_2/uu_000052.png training/gt_image_2/uu_road_000052.png\n",
      "training/image_2/uu_000053.png training/gt_image_2/uu_road_000053.png\n",
      "training/image_2/uu_000054.png training/gt_image_2/uu_road_000054.png\n",
      "training/image_2/uu_000055.png training/gt_image_2/uu_road_000055.png\n",
      "training/image_2/uu_000056.png training/gt_image_2/uu_road_000056.png\n",
      "training/image_2/uu_000057.png training/gt_image_2/uu_road_000057.png\n",
      "training/image_2/uu_000058.png training/gt_image_2/uu_road_000058.png\n",
      "training/image_2/uu_000059.png training/gt_image_2/uu_road_000059.png\n",
      "training/image_2/uu_000060.png training/gt_image_2/uu_road_000060.png\n",
      "training/image_2/uu_000061.png training/gt_image_2/uu_road_000061.png\n",
      "training/image_2/uu_000062.png training/gt_image_2/uu_road_000062.png\n",
      "training/image_2/uu_000063.png training/gt_image_2/uu_road_000063.png\n",
      "training/image_2/uu_000064.png training/gt_image_2/uu_road_000064.png\n",
      "training/image_2/uu_000065.png training/gt_image_2/uu_road_000065.png\n",
      "training/image_2/uu_000066.png training/gt_image_2/uu_road_000066.png\n",
      "training/image_2/uu_000067.png training/gt_image_2/uu_road_000067.png\n",
      "training/image_2/uu_000068.png training/gt_image_2/uu_road_000068.png\n",
      "training/image_2/uu_000069.png training/gt_image_2/uu_road_000069.png\n",
      "training/image_2/uu_000070.png training/gt_image_2/uu_road_000070.png\n",
      "training/image_2/uu_000071.png training/gt_image_2/uu_road_000071.png\n",
      "training/image_2/uu_000072.png training/gt_image_2/uu_road_000072.png\n",
      "training/image_2/uu_000073.png training/gt_image_2/uu_road_000073.png\n",
      "training/image_2/uu_000074.png training/gt_image_2/uu_road_000074.png\n",
      "training/image_2/uu_000075.png training/gt_image_2/uu_road_000075.png\n",
      "training/image_2/uu_000076.png training/gt_image_2/uu_road_000076.png\n",
      "training/image_2/uu_000077.png training/gt_image_2/uu_road_000077.png\n",
      "training/image_2/uu_000078.png training/gt_image_2/uu_road_000078.png\n",
      "training/image_2/uu_000079.png training/gt_image_2/uu_road_000079.png\n",
      "training/image_2/uu_000080.png training/gt_image_2/uu_road_000080.png\n",
      "training/image_2/uu_000081.png training/gt_image_2/uu_road_000081.png\n"
     ]
    }
   ],
   "source": [
    "imgsets_file = osp.join('./data/Kitti', '{}.txt'.format('train'))\n",
    "for line in open(imgsets_file):\n",
    "    line = line.strip()\n",
    "    print(line)\n",
    "    line = line.split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vEEnAaqx6Fh8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KITTIdataset(torch.utils.data.Dataset):\n",
    "    class_names = np.array(['background', 'road'])\n",
    "\n",
    "    def __init__(self, root, transform, split='train'): # root: \"./Kitti\"\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_path = []\n",
    "        self.ys = []\n",
    "        \n",
    "        imgsets_file = osp.join(root, '{}.txt'.format(split))\n",
    "        for did in open(imgsets_file):\n",
    "            did = did.strip()\n",
    "            did = did.split()\n",
    "            img_file = osp.join(root, 'data_road/{}'.format(did[0]))\n",
    "            lbl_file = osp.join(root, 'data_road/{}'.format(did[1]))\n",
    "            self.image_path.append(img_file)\n",
    "            self.ys.append(lbl_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image\n",
    "        img_file = self.image_path[index]\n",
    "        img = PIL.Image.open(img_file)\n",
    "        # img = np.array(img)\n",
    "        \n",
    "        # load label\n",
    "        lbl_file = self.ys[index]\n",
    "        lbl = PIL.Image.open(lbl_file)\n",
    "        lbl = np.array(lbl)\n",
    "        lbl[lbl == 255] = 1 # 0 is black 255 is white\n",
    "        \n",
    "        return self.transform(img), torch.from_numpy(lbl).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 1.0000,  1.0000,  1.0000,  ..., -0.1216, -0.2392, -0.2863],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.8353, -0.8196, -0.8039],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.7098, -0.6157, -0.4353],\n",
      "         ...,\n",
      "         [-0.4510, -0.4431, -0.3804,  ..., -0.2549, -0.2549, -0.2471],\n",
      "         [-0.4196, -0.4275, -0.3725,  ..., -0.2549, -0.2627, -0.2706],\n",
      "         [-0.3882, -0.3882, -0.4745,  ..., -0.2235, -0.2392, -0.2549]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  1.0000,  ...,  0.0431,  0.0431, -0.1216],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.4039, -0.4980, -0.4824],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.7725, -0.5922, -0.3020],\n",
      "         ...,\n",
      "         [-0.3490, -0.3569, -0.3725,  ..., -0.2235, -0.2471, -0.2706],\n",
      "         [-0.4196, -0.4431, -0.4510,  ..., -0.2471, -0.2392, -0.2549],\n",
      "         [-0.4510, -0.4353, -0.4118,  ..., -0.2627, -0.2627, -0.2627]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  1.0000,  ..., -0.6235, -0.5843, -0.5765],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.6471, -0.6549, -0.6784],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.8275, -0.8824, -0.8431],\n",
      "         ...,\n",
      "         [-0.2863, -0.3020, -0.3098,  ..., -0.2471, -0.2471, -0.2627],\n",
      "         [-0.4353, -0.4353, -0.4431,  ..., -0.2471, -0.2471, -0.2549],\n",
      "         [-0.4745, -0.4431, -0.4431,  ..., -0.2471, -0.2471, -0.2471]]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "train_dataset = KITTIdataset(root = './data/Kitti', split = 'train', transform = transform)\n",
    "val_dataset = KITTIdataset(root = './data/Kitti', split = 'val', transform = transform)\n",
    "\n",
    "print(train_dataset[0])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation matric (mIoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) +\n",
    "        label_pred[mask], minlength=n_class**2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "def compute_mean_iou(label_trues, label_preds, n_class):\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iou = np.nanmean(iu)\n",
    "    \n",
    "    return mean_iou\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wOtKS8ll6Fh-"
   },
   "source": [
    "# Define the Network\n",
    "\n",
    "- FCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_class=3):\n",
    "        super(FCN, self).__init__()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.num_class = num_class\n",
    "    \n",
    "        #3->64 2 \n",
    "        #64->128 2\n",
    "        #128->256 3 conv->relu->conv->relu->conv->relu => Predict 3\n",
    "        #256->512 3 conv->relu->conv->relu->conv->relu => Predict 2\n",
    "        #512->512 3 conv->relu->conv->relu->conv->relu\n",
    "        #512->4096 2 conv->relu->conv->relu => Predict 1\n",
    "        \n",
    "        ## conv1\n",
    "        self.features1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding =1),\n",
    "            nn.ReLU())\n",
    "        ## pool1\n",
    "        \n",
    "        ## conv2\n",
    "        self.features2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding = 1),\n",
    "            nn.ReLU())\n",
    "        ## pool2\n",
    "        \n",
    "        ## conv3\n",
    "        self.features3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1))\n",
    "        ## pool3\n",
    "\n",
    "        ## conv4\n",
    "        self.features4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1))\n",
    "        ## pool4\n",
    "        \n",
    "        ## conv5\n",
    "        self.features5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1))\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "        \n",
    "        #4096->4096->num_class\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(512, 4096, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(4096, 4096, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(4096, self.num_class, 1))\n",
    "        \n",
    "        \n",
    "        ## pool 5\n",
    "        ## upsampling transposed convolution\n",
    "        self.upscore2 = nn.ConvTranspose2d(self.num_class, self.num_class, kernel_size=4, stride=2, bias=False)\n",
    "        self.upscore4 = nn.ConvTranspose2d(self.num_class, self.num_class, kernel_size=4, stride=2, bias=False)\n",
    "        self.upscore8 = nn.ConvTranspose2d(self.num_class, self.num_class, kernel_size=16, stride=8, bias=False)\n",
    "        \n",
    "        self.score_pool4 = nn.Conv2d(512, self.num_class, 1)\n",
    "        self.score_pool3 = nn.Conv2d(256, self.num_class, 1)\n",
    "\n",
    "        self.softmax = nn.Softmax2d()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.features1(x)\n",
    "        pool1 = self.maxpool(x1)\n",
    "\n",
    "        x2 = self.features2(pool1)\n",
    "        pool2 = self.maxpool(x2)\n",
    "\n",
    "        x3 = self.features3(pool2)\n",
    "        pool3 = self.maxpool(x3)\n",
    "\n",
    "        x4 = self.features4(pool3)\n",
    "        pool4 = self.maxpool(x4)\n",
    "\n",
    "        x5 = self.features5(pool4)\n",
    "        pool5 = self.maxpool(x5)\n",
    "\n",
    "\n",
    "        predict1 = self.classifier(pool5)\n",
    "        \n",
    "        deconv1 = self.upscore2(predict1)\n",
    "        predict2 = self.score_pool4(pool4)\n",
    "        predict2 = predict2[:, :, 5:5 + deconv1.size()[2], 5:5 + deconv1.size()[3]]\n",
    "        add1 = torch.add(deconv1, predict2)\n",
    "\n",
    "        deconv2 = self.upscore4(add1)\n",
    "        predict3 = self.score_pool3(pool3)\n",
    "        predict3 = predict3[:, :, 9:9 + deconv2.size()[2], 9:9 + deconv2.size()[3]]\n",
    "        add2 = torch.add(deconv2, predict3)\n",
    "\n",
    "\n",
    "        deconv3 = self.upscore8(add2)\n",
    "        out = deconv3[:, :, 33:33 + x.size()[2], 33:33 + x.size()[3]]\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN(num_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch : 0\n",
      "batch : 0, loss : 0.6931928396224976\n",
      "batch : 20, loss : 0.6930980682373047\n",
      "batch : 40, loss : 0.6929555535316467\n",
      "batch : 60, loss : 0.6926196217536926\n",
      "batch : 80, loss : 0.6921765804290771\n",
      "batch : 100, loss : 0.6893310546875\n",
      "batch : 120, loss : 0.6705916523933411\n",
      "batch : 140, loss : 0.6463431715965271\n",
      "batch : 160, loss : 0.6237552762031555\n",
      "batch : 180, loss : 0.610308825969696\n",
      "batch : 200, loss : 0.619084894657135\n",
      "batch : 220, loss : 0.6027741432189941\n",
      "batch : 240, loss : 0.6110655069351196\n",
      "val loss : 0.590250626206398, mean_iou : 0.3960756046232154\n",
      "Best model saved\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './road_sample1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m         torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39m./model_best.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m     \u001b[39m## visualization\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     img \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39m./road_sample1.png\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m     visualization(model, img, epoch)\n\u001b[1;32m     62\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished Training\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/jupyter/lib/python3.11/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3226\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3227\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3228\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './road_sample1.png'"
     ]
    }
   ],
   "source": [
    "training_epochs = 5\n",
    "best_iou = 0\n",
    "num_class = len(train_loader.dataset.class_names)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    print ('current epoch : %d'%(epoch))\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # load data, forward\n",
    "        # data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        score = model(data)\n",
    "\n",
    "        loss = criterion(score, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 20 ==0:\n",
    "            print ('batch : {}, loss : {}'.format(batch_idx, loss.item()))\n",
    "\n",
    "        \n",
    "    #validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    metrics = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # load data, forward\n",
    "            # data, target = data.cuda(), target.cuda()\n",
    "            score = model(data)\n",
    "\n",
    "            # calc val loss, accuracy\n",
    "            loss = criterion(score, target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, lbl_pred = score.max(1)\n",
    "            lbl_pred = lbl_pred.cpu().numpy()  \n",
    "            lbl_true = target.cpu().numpy()\n",
    "\n",
    "            for lt, lp in zip(lbl_true, lbl_pred): # lbl_true, lbl_pred: [batch, h, w]\n",
    "                tmp = compute_mean_iou(lt, lp, num_class)\n",
    "                metrics.append(tmp)\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    metrics = np.mean(metrics)\n",
    "    \n",
    "    print ('val loss : {}, mean_iou : {}'.format(val_loss, metrics))\n",
    "\n",
    "    ##save model\n",
    "    if best_iou < metrics:\n",
    "        best_iou = metrics\n",
    "        print(\"Best model saved\")\n",
    "        torch.save(model.state_dict(), './model_best.pth')\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. U-Net implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_class=3):\n",
    "        super(UNet,self).__init__()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.num_class = num_class\n",
    "    \n",
    "        self.down_layer1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(64))\n",
    "        \n",
    "        self.down_layer2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(128))\n",
    "        \n",
    "        self.down_layer3 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(256))\n",
    "\n",
    "        self.pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        self.mid_layer = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(256))\n",
    "        \n",
    "        \n",
    "        self.up_layer1 = nn.Sequential(nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                                    nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(256))\n",
    "        self.up_layer2 = nn.Sequential(nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                                    nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(256))\n",
    "        self.up_layer3 = nn.Sequential(nn.ConvTranspose2d(384, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                                    nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(256))\n",
    "        \n",
    "        self.fc_layer = nn.Linear(128*256*128,num_class)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc_layer.weight.data)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.down_layer1(x)\n",
    "        x1 = self.pool_layer(x)\n",
    "        x1 = self.down_layer2(x1)\n",
    "        x2 = self.pool_layer(x1)\n",
    "        x2 = self.down_layer3(x2)\n",
    "        x3 = self.pool_layer(x2)\n",
    "        \n",
    "        x4 = self.mid_layer(x3)\n",
    "        \n",
    "        x5 = self.up_layer1(x4)\n",
    "        x6 = self.up_layer2(torch.cat((x5, x2), dim=1))\n",
    "        x7 = self.up_layer3(torch.cat((x6, x1), dim=1))\n",
    "        \n",
    "        out = self.fc_layer(x7.view((1,-1)))\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "seg_kitti_ans.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
